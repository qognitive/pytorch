{% import 'common.yml.j2' as common %}
{% import 'upload.yml.j2' as upload %}

{%- block name -%}
# Template is at:    .github/templates/linux_binary_build_workflow.yml.j2
# Generation script: .github/scripts/generate_ci_workflows.py
name: !{{ build_environment }}
{%- endblock %}


on:
  push:
  workflow_dispatch:

env:
  # Needed for conda builds
  {%- if "aarch64" in build_environment %}
  ALPINE_IMAGE: "arm64v8/alpine"
  {%- elif "s390x" in build_environment %}
  ALPINE_IMAGE: "docker.io/s390x/alpine"
  {%- else %}
  ALPINE_IMAGE: "alpine"
  {%- endif %}
  AWS_DEFAULT_REGION: us-east-1
  BINARY_ENV_FILE: /tmp/env
  BUILD_ENVIRONMENT: !{{ build_environment }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  PR_NUMBER: ${{ github.event.pull_request.number }}
  PYTORCH_FINAL_PACKAGE_DIR: /artifacts
  PYTORCH_ROOT: /pytorch
  SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
  SKIP_ALL_TESTS: 1
!{{ common.concurrency(build_environment) }}

jobs:
{%- for config in build_configs %}
  !{{ config["build_name"] }}-build:
    uses: ./.github/workflows/_binary-build-linux.yml
    with:!{{ upload.binary_env_as_input(config) }}
      {%- if "aarch64" in build_environment %}
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      runs_on: linux.arm64.m7g.4xlarge.ephemeral
      ALPINE_IMAGE: "arm64v8/alpine"
      {%- elif "s390x" in build_environment %}
      runs_on: linux.s390x
      ALPINE_IMAGE: "docker.io/s390x/alpine"
      timeout-minutes: 420
      {%- elif "conda" in build_environment and config["gpu_arch_type"] == "cuda" %}
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      runs_on: linux.24xlarge.ephemeral
      ALPINE_IMAGE: "alpine"
      {%- else %}
      ALPINE_IMAGE: "alpine"
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      {%- endif %}
      build_name: !{{ config["build_name"] }}
      build_environment: !{{ build_environment }}
      {%- if config.pytorch_extra_install_requirements is defined and config.pytorch_extra_install_requirements|d('')|length > 0  %}
      PYTORCH_EXTRA_INSTALL_REQUIREMENTS: !{{ config.pytorch_extra_install_requirements }}
      {%- endif %}
      {%- if config["gpu_arch_type"] == "cuda-aarch64" %}
      timeout-minutes: 420
      {%- endif %}
    secrets:
      github-token: ${{ secrets.GITHUB_TOKEN }}

  {%- if config["gpu_arch_type"] != "cuda-aarch64" %}
  !{{ config["build_name"] }}-test:  # Testing
    needs:
      - !{{ config["build_name"] }}-build
    {%- if config["gpu_arch_type"] not in ["rocm", "xpu"] %}
    uses: ./.github/workflows/_binary-test-linux.yml
    with:!{{ upload.binary_env_as_input(config) }}
      build_name: !{{ config["build_name"] }}
      build_environment: !{{ build_environment }}
      {%- if "aarch64" in build_environment %}
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      runs_on: linux.arm64.2xlarge
      ALPINE_IMAGE: "arm64v8/alpine"
      {%- elif "s390x" in build_environment %}
      runs_on: linux.s390x
      ALPINE_IMAGE: "docker.io/s390x/alpine"
      {%- elif config["gpu_arch_type"] == "rocm" %}
      runs_on: linux.rocm.gpu
      {%- elif config["gpu_arch_type"] == "cuda" %}
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      runs_on: linux.4xlarge.nvidia.gpu
      ALPINE_IMAGE: "alpine"
      {%- else %}
      ALPINE_IMAGE: "alpine"
      runner_prefix: "${{ needs.get-label-type.outputs.label-type }}"
      runs_on: linux.4xlarge
      {%- endif %}
    secrets:
      github-token: ${{ secrets.GITHUB_TOKEN }}
    {%- elif config["gpu_arch_type"] == "xpu" %}
    runs-on: linux.idc.xpu
    timeout-minutes: !{{ common.timeout_minutes }}
    !{{ upload.binary_env(config) }}
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Setup XPU
        uses: ./.github/actions/setup-xpu
      - name: configure aws credentials
        id: aws_creds
        uses: aws-actions/configure-aws-credentials@v1.7.0
        with:
          role-to-assume: arn:aws:iam::308535385114:role/gha_workflow_s3_and_ecr_read_only
          aws-region: us-east-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      - uses: !{{ common.download_artifact_action }}
        name: Download Build Artifacts
        with:
          name: !{{ config["build_name"] }}
          path: "${{ runner.temp }}/artifacts/"
      !{{ common.checkout(deep_clone=False, directory="pytorch") }}
      - name: Pull Docker image
        uses: pytorch/test-infra/.github/actions/pull-docker-image@main
        with:
          docker-image: !{{ config["container_image"] }}
      - name: Test Pytorch binary
        uses: ./pytorch/.github/actions/test-pytorch-binary
      - name: Teardown XPU
        uses: ./.github/actions/teardown-xpu
    {%- else %}
    runs-on: linux.rocm.gpu
    timeout-minutes: !{{ common.timeout_minutes }}
    !{{ upload.binary_env(config) }}
    steps:
      - name: Setup ROCm
        uses: ./.github/actions/setup-rocm
      - uses: !{{ common.download_artifact_action }}
        name: Download Build Artifacts
        with:
          name: !{{ config["build_name"] }}
          path: "${{ runner.temp }}/artifacts/"
      !{{ common.checkout(deep_clone=False, directory="pytorch") }}
      - name: ROCm set GPU_FLAG
        run: |
          echo "GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri --group-add video --group-add daemon" >> "${GITHUB_ENV}"
      - name: Pull Docker image
        uses: pytorch/test-infra/.github/actions/pull-docker-image@main
        with:
          docker-image: !{{ config["container_image"] }}
      - name: Test Pytorch binary
        uses: ./pytorch/.github/actions/test-pytorch-binary
      - name: Teardown ROCm
        uses: ./.github/actions/teardown-rocm
    {%- endif %}
  {%- endif %}

{%- if branches == "nightly" %}
  upload-to-gemfury:
    needs:
      - !{{ config["build_name"] }}-test
    runs-on: ubuntu-22.04
    steps:
      - name: Download Build Artifacts
        id: download-artifacts
        uses: actions/download-artifact@v4.1.7
        with:
          name: !{{ config["build_name"] }}
          path: "${{ runner.temp }}/artifacts/"
      - name: install gemfury CLI
        run: |
          echo "deb [trusted=yes] https://apt.fury.io/cli/ * *" | sudo tee /etc/apt/sources.list.d/fury-cli.list
          sudo apt-get update
          sudo apt-get install fury-cli
        shell: bash
      - name: print ls of artifacts
        run: ls ${{ runner.temp }}/artifacts
        shell: bash
      - name: push wheel to gemfury
        run: fury push "${{ runner.temp }}/artifacts/$(ls ${{ runner.temp }}/artifacts -1)" --api-token ${{ secrets.GEMFURY_FULL_TOKEN }}
        shell: bash
{%- endif %}
{% endfor %}
